\documentclass[11pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{enumitem}
\usepackage{graphicx}
\usetheme{EastLansing}
\author{Wang Yao}
\begin{document}
	\title{Data Science Study Notes}
	\subtitle{Some Common Interview Questions}
	%\logo{}
	%\institute{}
	%\date{}
	%\subject{}
	%\setbeamercovered{transparent}
	%\setbeamertemplate{navigation symbols}{}
	\begin{frame}[plain]
	\maketitle
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{block}{Question:}
	What are basic methods to reduce dimension?
\end{block}
\begin{itemize}
	\item Use correlation test to remove correlated numerical variables with business understanding
	\begin{itemize}
		\item A correlation coefficient measures the extent to which two variables tend to change together. The coefficient describes both  strength and direction of the relationship
		\item \textbf{Pearson correlation} evaluates the \textbf{linear relationship} between two \textbf{continuous variables}
		\item \textbf{Spearman rank-order correlation} evaluates the \textbf{monotonic relationship} between two continuous or \textbf{ordinal} variables. In a \textbf{monotonic relationship}, the variables tend to change together, but not necessarily at a constant rate. 
		\item The \textbf{Pearson} and \textbf{Spearman} coefficient are both nearly $0$ \textbf{does not} implies there is no strong relationship between two variables
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item  Use Chi-Square test to remove correlated categorical variables with business understanding
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item Use PCA and pick the components which can explain the maximum variance in the data set. 
	\begin{itemize}
		\item \textbf{Principal Component Analysis} performs a linear mapping of the data to a lower dimensional space in such a way that the variance of data in the low-dimensional representation is maximized. 
		\item In practice, the \textbf{convariance}(sometimes the \textbf{correlation}) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues can now be used to reconstruct a large fraction of the variance of original data.
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item \textbf{Kernel PCA}
	\begin{itemize}
		\item \textbf{PCA} can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in data.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item \textbf{Graph-based kernel PCA} is one of techniques that try to learn the kernel using semidefinite programming, instead of defining a fixed kernel.
	\begin{itemize}
		\item \textbf{PCA} can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in data.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item \textbf{Non-negative matrix factorization(NMF)}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LDA, GDA and Autoencoder in dimension reduction}
\begin{itemize}
	\item \textbf{Linear discriminant analysis(LDA)}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LDA, GDA and Autoencoder in dimension reduction}
\begin{itemize}
	\item \textbf{Generalized discriminant analysis(GDA)}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LDA, GDA and Autoencoder in dimension reduction}
\begin{itemize}
	\item \textbf{Autoencoder}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Kernel method}
\begin{itemize}
	\item In its simplest form, the \textbf{Kernel methods} means transforming data into another dimension that has a clear dividing margin between classes of data 
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\end{document}