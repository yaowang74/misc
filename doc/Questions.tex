\documentclass[11pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{enumitem}
\usepackage{graphicx}
\usetheme{EastLansing}
\author{Wang Yao}
\begin{document}
	\title{Data Science Study Notes}
	\subtitle{Some Common Interview Questions}
	%\logo{}
	%\institute{}
	%\date{}
	%\subject{}
	%\setbeamercovered{transparent}
	%\setbeamertemplate{navigation symbols}{}
	\begin{frame}[plain]
	\maketitle
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{block}{Question:}
	What are basic methods to reduce dimension?
\end{block}
\begin{itemize}
	\item Use correlation test to remove correlated numerical variables with business understanding
	\begin{itemize}
		\item A correlation coefficient measures the extent to which two variables tend to change together. The coefficient describes both  strength and direction of the relationship
		\item \textbf{Pearson correlation} evaluates the \textbf{linear relationship} between two \textbf{continuous variables}
		\item \textbf{Spearman rank-order correlation} evaluates the \textbf{monotonic relationship} between two continuous or \textbf{ordinal} variables. In a \textbf{monotonic relationship}, the variables tend to change together, but not necessarily at a constant rate. 
		\item The \textbf{Pearson} and \textbf{Spearman} coefficient are both nearly $0$ \textbf{does not} implies there is no strong relationship between two variables
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item  Use Chi-Square test to remove correlated categorical variables with business understanding
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item Use PCA and pick the components which can explain the maximum variance in the data set. 
	\begin{itemize}
		\item \textbf{Principal Component Analysis} performs a linear mapping of the data to a lower dimensional space in such a way that the variance of data in the low-dimensional representation is maximized. 
		\item In practice, the \textbf{convariance}(sometimes the \textbf{correlation}) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues can now be used to reconstruct a large fraction of the variance of original data.
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item \textbf{Kernel PCA}
	\begin{itemize}
		\item \textbf{PCA} can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in data.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item \textbf{Graph-based kernel PCA} is one of techniques that try to learn the kernel using semidefinite programming, instead of defining a fixed kernel.
	\begin{itemize}
		\item \textbf{PCA} can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in data.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item \textbf{Non-negative matrix factorization(NMF)}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LDA, GDA and Autoencoder in dimension reduction}
\begin{itemize}
	\item \textbf{Linear discriminant analysis(LDA)}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LDA, GDA and Autoencoder in dimension reduction}
\begin{itemize}
	\item \textbf{Generalized discriminant analysis(GDA)}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LDA, GDA and Autoencoder in dimension reduction}
\begin{itemize}
	\item \textbf{Autoencoder}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Kernel method}
\begin{block}{Question:}
	What’s the “kernel trick” and how is it useful?
\end{block}
\begin{block}{Answer:}
	The Kernel trick involves kernel functions that can enable in higher-dimension spaces without explicitly calculating the coordinates of points within that dimension. This allows them the very useful attribute of calculating the coordinates of higher dimensions while being computationally cheaper than the explicit calculation of said coordinates. Many algorithms can be expressed in terms of inner products. Using the kernel trick enables us effectively run algorithms in a high-dimensional space with lower-dimensional data.
	
	 In its simplest form, the \textbf{Kernel methods} means transforming data into another dimension that has a clear dividing margin between classes of data 
\end{block}
%\begin{itemize}
%	\item
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Time Series and related}
\begin{block}{Question:}
	How can you check if a data set or time series is Random?
\end{block}
\begin{block}{Answer:}
	To check whether a dataset is random or not, use the lag plot. If the lag plot for the given dataset does not show any structure then it is random.
\end{block}
\begin{itemize}
%	\item A lag plot is a special type of scatter plot with the two variables $(X,Y)$ “lagged.” 
	\item One set of observations in a time series is plotted (lagged) against a set of later data. The kth lag is the time period that happened $k$ time points before time $i$. e.g.: $Lag_1(Y2) = Y1$ and $Lag_4(Y9) = Y5$.
	\begin{itemize}
		\item Model suitability and outliers
		\item Randomness (data without a pattern), Serial correlation (where error terms in a time series transfer from one period to another).
		\item Seasonality (periodic fluctuations in time series data that happens at regular periods).
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Time Series and related}
\begin{block}{Question:}
	How can you deal with different types of seasonality in time series modeling?
\end{block}
\begin{block}{Answer:}
	Seasonality in time series occurs when time series shows a repeated pattern over time. E.g., stationary sales decreases during holiday season, air conditioner sales increases during the summers etc.
	
	Seasonality makes your time series non-stationary because average value of the variables at different time periods. Differentiating a time series is generally known as the best method of removing seasonality from a time series. Seasonal differencing can be defined as a numerical difference between a particular value and a value with a periodic lag.
	
	Model the seasonal component directly and subtract it from observations.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item 
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Clustering and related}
\begin{block}{Question:}
	How will you define the number of clusters in a clustering algorithm?
\end{block}
\begin{block}{Answer:}
	Though the Clustering Algorithm is not specified, this question is mostly in reference to $K$-Means clustering where $K$ defines the number of clusters. The objective of clustering is to group similar entities in a way that the entities within a group are similar to each other but the groups are different from each other
	
	Within Sum of squares is generally used to explain the homogeneity within a cluster. If you plot WSS for a range of number of clusters, you will get "Elbow Curve", Use "Bending Point" as $K$ in $K$-means, where within groups sum of squares does not decrease significantly.
\end{block}
%\begin{itemize}
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item 
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Clustering and related}
\begin{block}{Question:}
	How is $KNN$ different from $K$-means clustering?
\end{block}
\begin{block}{Answer:}
	$K$-Nearest Neighbors is a supervised classification algorithm, while $K$-means clustering is an unsupervised clustering algorithm. While the mechanisms may seem similar at first, what this really means is that in order for $K$-Nearest Neighbors to work, you need labeled data you want, to classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires only a set of unlabeled points and a threshold: the algorithm will take unlabeled points and gradually learn how to cluster them into groups by computing the mean of the distance between different points.
	
	The critical difference here is that $KNN$ needs labeled points and is thus supervised learning, while $K$-means doesn’t — and is thus unsupervised learning.
\end{block}
%\begin{itemize}
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item 
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Linear model and related}
\begin{block}{Question:}
	What’s the assumption of linear regression?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item Linear relationship between dependent and independent variables
		\item Statistical independence of the errors (no correlation between consecutive errors in the case of time series data)
		\item Homoscedasticity (constant variance) of the errors
		\item Normality of the error distribution.
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item 
%	\item 
%	\item
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Linear model and related}
\begin{block}{Question:}
	How to check if a distribution is close to Normal? Why would you want to check it? What is a QQ Plot?
\end{block}
\begin{block}{Answer:}
	A Q-Q plot or quantile-quantile plot, is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a Normal or exponential. 
	
	For example, if we run a statistical analysis that assumes our independent variable is Normally distributed, we can use a Normal Q-Q plot to check that assumption. It’s just a visual check, not an airtight proof, so it is somewhat subjective. 
	
	But it allows us to see at-a-glance if our assumption is plausible, and if not, how the assumption is violated and what data points contribute to the violation.
\end{block}
%\begin{itemize}
%	\item 
%	\item
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	Can you cite some examples where a false negative important than a false positive?
\end{block}
\begin{block}{Answer:}
	Assume there is an airport which has received high security threats and based on certain characteristics they try to identify whether a particular passenger can be a threat or not. Due to shortage of staff they decided to scan passenger being predicted as risk positives by their predictive model.
	What will happen if a true threat customer is being flagged as non-threat by airport model?
	
	Another example can be judicial system. What if Jury or judge decide to make a criminal go free?
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of sensitivity?
\end{block}
\begin{block}{Answer:}
	Sensitivity is also called true positive rate, recall, which measures the proportion of actual positives that are correctly identified as such(e.g., the percentage of sick people who are correctly identified as having the condition).
	
	Specificity is also called true negative rate, which measures the proportion of actual negatives that are correctly identified as such(e.g., the percentage of healthy people who are correctly identified as not having the condition).
	
	Sensitivity quantifies the avoiding of false negatives and specificity does the same for false positives. For any test, there is usually a trade-off between the measures
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of sensitivity?
\end{block}
\begin{block}{Answer:}
	Equivalently, in medical tests sensitivity is the extent to which actual positives are not overlooked.
	
	Specificity is the extent to which actual negatives are classified as such.
	
	Thus a highly sensitive test rarely overlooks an actual positive; a highly specific test rarely registers a positive classification for anything that is not the target of testing.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of sensitivity?
\end{block}
\begin{block}{Answer:}
	Sensitivity refers to the test's ability to correctly detect ill patients who do have the condition. In the example of a medical test used to identify a disease, the sensitivity of the test is the proportion of people who test positive for the disease among those who have the disease.
	\begin{align*}
		\text{sensitivity}=&\frac{\text{num. of true positives}}{\text{num. of true positives} +\text{num. of false negatives}}\\
		=&\frac{\text{num. of true positives}}{\text{total num. of sick individuals in population}}\\
		=& \text{prob. of a positive test given that the patient has the disease}
	\end{align*}

\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of  specificity?
\end{block}
\begin{block}{Answer:}
	Specificity relates to the test's ability to correctly reject healthy patients without a condition. In the example of a medical test for diagnosing a disease, specificity of a test is the proportion of healthy patients known not to have the disease, who will test negative for it.
	\begin{align*}
	\text{specificity}=&\frac{\text{num. of true negatives}}{\text{num. of true negatives} +\text{num. of false positives}}\\
	=&\frac{\text{num. of true negatives}}{\text{total num. of well individuals in population}}\\
	=& \text{prob. of a negative test given that the patient is well}
	\end{align*}
	
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of  specificity?
\end{block}
\begin{block}{Answer:}
	A positive result in a test with high specificity is useful for ruling in disease. The test rarely gives positive results in healthy patients. A test with $100\%$ specificity will read negative, and accurately exclude disease from all healthy patients. A positive result signifies a high probability of the presence of disease.
	
	A test with a higher specificity has a lower type I error rate.
	
	The tradeoff between specificity and sensitivity is explored in ROC analysis as a trade off between TPR and FPR
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of  specificity?
\end{block}
\begin{block}{Answer:}
	A positive result in a test with high specificity is useful for ruling in disease. The test rarely gives positive results in healthy patients. A test with $100\%$ specificity will read negative, and accurately exclude disease from all healthy patients. A positive result signifies a high probability of the presence of disease.
	
	A test with a higher specificity has a lower type I error rate.
	
	The tradeoff between specificity and sensitivity is explored in ROC analysis as a trade off between TPR and FPR
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of sensitivity?
\end{block}
\begin{block}{Answer:}
	The number of real positive(negative) events: $\textbf{P}$($\textbf{N}$)
	
	True positive(negative): $\textbf{TP}$($\textbf{TN}$); False positive(negative): $\textbf{FP}$($\textbf{FN}$)
	Sensitivity, Recall, Hit Rate or True Positive Rate(\textbf{TPR})
	\[
		\textbf{TPR} = \frac{\textbf{TP}}{\textbf{P}} = \frac{\textbf{TP}}{\textbf{TP}+\textbf{FN}}
		=1 - \textbf{FNR}
	\]
	Specificity, Selectivity or True Negative Rate(\textbf{TNR})
	\[
		\textbf{TNR} = \frac{\textbf{TN}}{\textbf{N}} = \frac{\textbf{TN}}{\textbf{FP}+\textbf{TN}}
		=1 - \textbf{FPR}
	\]
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
%\begin{block}{Question:}
%	What do you calculate Precision?
%\end{block}
\begin{block}{Answer:}
	Precision or Positive Predictive Value(\textbf{PPV})
	\[
		\textbf{PPV} = \frac{\textbf{TP}}{\textbf{TP}+\textbf{FP}}
	\]
	Negative Predictive Value(\textbf{NPV})
	\[
		\textbf{NPV} = \frac{\textbf{TN}}{\textbf{TN}+\textbf{FN}}
	\]
	False Positive Rate(\textbf{FPR})
	\[
		\textbf{FPR} = \frac{\textbf{FP}}{\textbf{N}} = \frac{\textbf{FP}}{\textbf{FP}+\textbf{TN}}
	\]
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you calculate model accuracy and $\textbf{F}_1$ score?
\end{block}
\begin{block}{Answer:}
	Accuracy(\textbf{ACC})
	\[
		\textbf{ACC} = \frac{\textbf{TP}+\textbf{TN}}{\textbf{P}+\textbf{N}} = \frac{\textbf{TP}+\textbf{TN}}{\textbf{TP}+\textbf{TN} + \textbf{FP}+\textbf{FN}}
	\]
	$\textbf{F}_1$ Score(harmonic mean of Precision and Recall)
	\[
		\textbf{F}_1 = 2\times\frac{\textbf{TPR}\times\textbf{PPV}}{\textbf{TPR} + \textbf{PPV}} = \frac{2\textbf{TP}}{2\textbf{TP}+\textbf{FP}+\textbf{FN}}
	\]
	with results tending to 1 being the best and 0 being the worst, would use it in classification tests where \textbf{TN} don't matter much
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What is Bayes’ Theorem? How is it useful in a machine learning context?
\end{block}
\begin{block}{Answer:}
	Bayes’ Theorem gives you the posterior probability of an event given what is known as prior knowledge. It’s expressed as the \textbf{TPR} of a condition sample divided by the sum of \textbf{FPR} of the population and \textbf{TPR} of a condition. Say you had a $60\%$ chance of actually having the flu after a flu test, but out of people who had the flu, the test will be false $50\%$ of the time, and the overall population only has a $5\%$ chance of having the flu. What is your actual chance of having the flu after having a positive test?
	
	Bayes’ Theorem says that you have a (.6 * 0.05) (\textbf{TPR} of a Condition Sample) / (.6*0.05)(\textbf{TPR} of a Condition Sample) + (.5*0.95) (\textbf{FPR} of a Population)  = $5.94\%$ chance of getting a flu.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	An HIV test has a sensitivity of $99.7\%$ and a specificity of $98.5\%$. A subject from a population of prevalence $0.1\%$ receives a positive test result. What is the precision of the test (i.e the probability he is HIV positive)?
\end{block}
\begin{block}{Answer:}
	\begin{align*}
	&P(Actu_+\mid Pred_+)\\ &\quad= \frac{P( Pred_+\mid Actu_+)\times P(Actu_+)}{P( Pred_+\mid Actu_+)\times P(Actu_+) + P( Pred_+\mid Actu_-)\times P(Actu_-)}\\
	&\quad=\frac{sensitivity\times prevalence}{sensitivity\times prevalence + (1 - specificity)\times(1-prevalence)}\\
%	&\quad=\frac{0.997\times 0.001}{0.997\times 0.001 + 0.015\times 0.999} = 0.062
	\end{align*}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What’s the difference between Type I and Type II error?
\end{block}
\begin{block}{Answer:}
	Type I error is a false positive, while Type II error is a false negative. Briefly stated, Type I error means claiming something has happened when it hasn’t, while Type II error means that you claim nothing is happening when in fact something is.
	
	A clever way to think about this is to think of Type I error as telling a man he is pregnant, while Type II error means you tell a pregnant woman she isn’t carrying a baby.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance metrics and related}
\begin{block}{Question:}
	What are the limitations of $R^2$
\end{block}
\begin{block}{Answer:}
	$R^2$ is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in dependent variable that independent variables explain collectively. 
%	$R^2$ measures the strength of the relationship between your model and the dependent variable.
	$R^2$ cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots.
	$R^2$ does not indicate whether a regression model is adequate. You can have a low $R^2$ value for a good model, or a high $R^2$ value for a model that does not fit the data.
	$R^2$ does not measure goodness of fit.
	$R^2$ does not measure predictive error.
	$R^2$ does not allow you to compare models using transformed responses.
	$R^2$ does not measure how one variable explains another.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance and related}
\begin{block}{Question:}
	What’s the trade-off between bias and variance?
\end{block}
\begin{block}{Answer:}
	Bias is error due to erroneous or overly simplistic assumptions in the learning algorithm you’re using. This can lead to the model underfitting your data, making it hard to have high predictive accuracy and to generalize your knowledge from the training set to the test set.
	
	Variance is error due to too much complexity in the learning algorithm you’re using. This leads to the algorithm being highly sensitive to high degrees of variation in your training data, which can lead your model to overfit the data. You’ll be carrying too much noise from your training data for your model to be very useful for your test data.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance and related}
\begin{block}{Question:}
What’s the trade-off between bias and variance?
\end{block}
\begin{block}{Answer:}
The bias-variance decomposition essentially decomposes the learning error from any algorithm by adding the bias, the variance and a bit of irreducible error due to noise in the underlying dataset. Essentially, if you make the model more complex and add more variables, you’ll lose bias but gain some variance, in order to get the optimally reduced amount of error, you’ll have to tradeoff bias and variance. You don’t want either high bias or high variance in your model.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance and related}
\begin{block}{Question:}
	What’s the trade-off between bias and variance?
\end{block}
\begin{block}{Answer:}
	High variance typicaly means that we are overfitting to our training data, finding patterns and complexity that are a product of randomness as opposed to some real trend. Generally, a more complex or flexible model will tend to have high variance due to overfitting but lower bias because, averaged over several predictions, our model more accurately predicts the target variable. On the other hand, an underfit or oversimplified model, while having lower variance, will likely be more biased since it lacks the tools to fully capture trends in the data.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance and related}
\begin{block}{Question:}
	What’s Bootstrapping and bagging?
\end{block}
\begin{block}{Answer:}
	Bootstrap refers to random sampling with replacement. Bootstrap allows us to better understand bias and variance of the dataset. Bootstrap involves random sampling of small subset of data from the dataset, the selection of all example in the dataset has equal probability. 
	
	Bootstrap Aggregation, or Bagging for short, is a simple and very powerful ensemble method. Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance and related}
\begin{block}{Question:}
	What’s relationship between bagging and random forest?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item A sample from observation is selected randomly with replacement
		\item A subset of features are randomly selected to create a decision tree
		\item Trained many trees in parallel, using above procedures
		\item Outcome is given by aggregating predictions from all trees
	\end{itemize}
	Randomization increases bias but makes it possible to reduce overall variance through averaging: let $Z_1,\ldots Z_N$ be i.i.d. random variables
	\[
		Var\left(\frac{1}{N}\sum_{i}Z_i\right) = \frac{1}{N}Var(Z_i)
	\]
	Average models to reduce model variance
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance and related}
\begin{block}{Question:}
	What is Boosting?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item Weight all training samples equally, and train model on training set
		\item Compute error of model on training set
		\item Increase weights on training cases model gets wrong
		\item Train new model on re-weighted training set
		\item Re-compute errors on weighted training set
		\item Increase weights again on cases model gets wrong
	\end{itemize}
Repeat the process, final model: weighted prediction of each model. Aim to reduce bias, not variance
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance and related}
\begin{block}{Question:}
	Summarize difference between Boosting and Bagging?
\end{block}
\begin{block}{Answer:}
	Bagging:
	\begin{itemize}
		\item parallel ensemble: each model is built independently
		\item aim to decrease variance, not bias
		\item suitable for high variance low bias models (complex models)
		\item an example of a tree based method is RF, which develop fully grown trees(note that RF modifies the grown procedure to reduce the correlation between trees)
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance and related}
\begin{block}{Question:}
	Summarize difference between Boosting and Bagging?
\end{block}
\begin{block}{Answer:}
	Boosting:
	\begin{itemize}
		\item sequential ensemble: try to add new models that do well where previous models lack
		\item aim to decrease bias, not variance
		\item suitable for low variance high bias models
		\item an example of a tree based method is gradient boosting
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance and related}
\begin{block}{Question:}
	Summarize difference between Boosting and Bagging?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item Bagging doesn’t work so well with stable models(low variance). Boosting might still
		help
		\item Boosting might hurt performance on noisy datasets while bagging doesn't have this problem
		\item On average, boosting helps more than bagging, but it is also more
		common for boosting to hurt performance.
		\item Bagging is easier to parallelize
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance and related}
\begin{block}{Question:}
	Explain how a ROC curve works.
\end{block}
\begin{block}{Answer:}
	The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model validation and related}
\begin{block}{Question:}
	How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression.
\end{block}
\begin{block}{Answer:}
	If the values predicted by the model are far outside of the response variable range, this would immediately indicate poor estimation or model inaccuracy. If the values seem to be reasonable, examine the parameters; any of the following would indicate poor estimation or multi-collinearity: opposite signs of expectations, unusually large or small values, or observed inconsistency when the model is fed new data. Use the model for prediction by feeding it new data, and use the coefficient of determination ($R^2$) as a model validity measure mean squared error (MSE).
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confidence interval}
\begin{block}{Question:}
	What is confidence interval?
\end{block}
\begin{block}{Answer:}
	In statistics, a confidence interval is a type of interval estimate that is computed from the observed data. A confidence interval gives an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a given set of sample data.
	
	The confidence level is the probability that the interval produced by the method employed includes the true value of the parameter. Confidence intervals consist of a range of values (interval) that act as good estimates of the unknown population parameter.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confidence interval}
\begin{block}{Question:}
	Why is “Naive” Bayes naive?
\end{block}
\begin{block}{Answer:}
	Despite its practical applications, especially in text mining, Naive Bayes is considered “Naive” because it makes an assumption that is virtually impossible to see in real-life data: the conditional probability is calculated as the pure product of the individual probabilities of components. This implies the absolute independence of features — a condition probably never met in real life.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confidence interval}
\begin{block}{Question:}
	Explain the difference between L1 and L2 regularization.
\end{block}
\begin{block}{Answer:}
	$L_2$ regularization tends to spread error among all the terms, while $L_1$ is more binary/sparse, with many variables either being assigned a 1 or 0 in weighting. $L_1$ corresponds to setting a Laplacean prior on the terms, while $L_2$ corresponds to a Gaussian prior.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confidence interval}
\begin{block}{Question:}
	How is a decision tree pruned?
\end{block}
\begin{block}{Answer:}
	Pruning is what happens in decision trees when branches that have weak predictive power are removed in order to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can happen bottom-up and top-down, with approaches such as reduced error pruning and cost complexity pruning.
	
	Reduced error pruning is perhaps the simplest version: replace each node. If it doesn’t decrease predictive accuracy, keep it pruned. While simple, this heuristic actually comes pretty close to an approach that would optimize for maximum accuracy.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Long-tailed distribution}
\begin{block}{Question:}
	Explain what a long-tailed distribution is and provide examples. Why are they important in classification and regression problems?
\end{block}
\begin{block}{Answer:}
	In a skewed distribution with a long tail, a high frequency population is followed by a low frequency population, which gradually tails off asymptotically.
%	Rule of thumb: majority of occurrences (more than half, and when Pareto principles applies, $80\%$) are accounted for by the first $20\%$ items in the distribution. The least frequently occurring $80\%$ of items are more important as a proportion of the total population.
	
	Natural language:
	The frequency of any word is inversely proportional to its rank in the frequency table. 
%	The most frequent word will occur twice as often as the second most frequent, three times as often as the third most frequen.
	% “The” accounts for $7\%$ of all word occurrences (70000 over 1 million). “of” accounts for $3.5\%$, followed by “and”… Only 135 vocabulary items are needed to account for half the English corpus.
	
	Allocation of wealth among individuals: the larger portion of the wealth of any society is controlled by a smaller percentage of the people. 
	%File size distribution of Internet Traffic, Hard disk error rates, values of oil reserves in a field (a few large fields, many small ones), sizes of sand particles, sizes of meteorites.
	
	In classification and regression problems, this is a issue when using models that make assumptions on the linearity and need to apply a monotone transformation on the data (logarithm). When sampling, the data will become even more unbalanced.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Activation function in NN}
\begin{block}{Question:}
	What is the function of activation function in ANN?
\end{block}
\begin{block}{Answer:}
	To add the non-linear factor into the model.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sampling methods}
\begin{block}{Question:}
	What is sampling? How many sampling methods?
\end{block}
\begin{block}{Answer:}
	Sampling is that part of statistical practice concerned with the selection of an unbiased or random subset of individual observations within a population of individuals intended to yield some knowledge about the population of concern.
	
	There are four sampling methods: Simple Random (purely random), Systematic( every kth member of population), cluster (population divided into groups or clusters)and stratified (divided by exclusive groups or strata, sample from each group) samplings.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Loss function}
\begin{block}{Question:}
	What’s the loss function log-loss?
\end{block}
\begin{block}{Answer:}
	Log Loss is an evaluation metric used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier’s predictions.
	
	Maximizing the likelihood is the same as minimizing the cross entropy
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Data preprocessing} 
\begin{block}{Question:}
	How do data management procedures like missing data handling make selection bias worse?
\end{block}
\begin{block}{Answer:}
	Removing entire row in data even if one value is missing could achieve a selection bias if your values are not missing at random and they have some pattern. 
	
	Replacing missing values with mean of other available values might make your distribution biased e.g., standard deviation, correlation and regression are mostly dependent on the mean value of variables.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Data preprocessing}
\begin{block}{Question:}
	During analysis, how do you treat missing values?
\end{block}
\begin{block}{Answer:}
	If any patterns for missing values are identified the analyst has to concentrate on them as it could lead to interesting and meaningful business insights. If there are no patterns identified, then the missing values can be substituted with mean or median (imputation) or they can simply be ignored.
	
	If it is a categorical variable, the default value(using business insight) can be assigned.
	
	If you have a distribution of data coming, for normal distribution give the mean value.
	
	If $80\%$ or more of the values for a variable are missing then you would be dropping the variable.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Data preprocessing}
\begin{block}{Question:}
	How would you handle an imbalanced dataset?
\end{block}
\begin{block}{Answer:}
	An imbalanced dataset is when you have, for example, a classification test and $90\%$ of the data is in one class. That leads to problems: an accuracy of $90\%$ can be skewed if you have no predictive power on the other category of data! Here are a few tactics to get over the hump:
	
	1- Collect more data to even the imbalances in the dataset.
	
	2- Downsample or oversample the dataset to correct for imbalances.
	
	3- Try a different algorithm altogether on your dataset.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Statistics: $p$-value and related}
\begin{block}{Question:}
	What does $p$-value signify about the statistical data?
\end{block}
\begin{block}{Answer:}
	$p$-value is used to determine the significance of results after a hypothesis test in statistics. $p$-value helps the readers to draw conclusions and is always between 0 and 1. Let significance level is 0.05, then 
	$p$-value $> 0.05$ denotes weak evidence against the null hypothesis which means the null hypothesis cannot be rejected.
	$p$-value $<= 0.05$ denotes strong evidence against the null hypothesis which means the null hypothesis can be rejected.
	$p$-value $=0.05$ is the marginal value indicating it is possible to go either way.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Statistics: Expected value and Mean value}
\begin{block}{Question:}
	Are expected value and mean value different?
\end{block}
\begin{block}{Answer:}
	They are not different but the terms are used in different contexts. Mean is generally referred when talking distribution or sample population while expected value is generally referred in a random variable context.
	
	For Sampling Data: Mean value is the only value that comes from the sampling data.
	Expected Value is the mean of all the means i.e. the value that is built from multiple samples. Expected value is the population mean.
	
	For Distributions: Mean value and Expected value are same irrespective of the distribution, under the condition that the distribution is in the same population.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Statistics: Distribution}
\begin{block}{Question:}
	What’s the differences between the poission distribution and normal distribution?
\end{block}
\begin{block}{Answer:}
		\begin{itemize}
			\item A Poisson distribution is discrete while a normal distribution is continuous
			\item A Poisson random variable is always >= 0.
			\item When the mean of a Poisson distribution is large, it becomes similar to a normal distribution.
		\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Statistics: Experimental design and related}
\begin{block}{Question:}
	Design an experiment to figure out which web design alternative to use. Assume there have been no other experiments done and there is no knowledge of the user behavior.
\end{block}
\begin{block}{Answer:}
	Some other issues:
	1. Identify potential biases due to interactions across pages. 
	2. Perform a A/A test which implies testing two random samples of visitors, and check if the distribution and metric of choice does not have a statistically significant difference. This will ensure the fairness of the $A/B$ test. An $A/A$ test ensures that your audience does not have a particular skew or bias and a randomized selection for an $A/B$ test will be statistically relevant
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Recommendation system and related}
\begin{block}{Question:}
	If you were a data scientist at a web company that sells shoes, how would you build a system that recommends shoes to a visitor? (Verizon interview)
\end{block}
\begin{block}{Answer:}
	If it is a new company that does not have much historical user data, go with item­ to item similarity. If the number of different items/shoes is extremely large, consider using matrix factorization techniques to reduce the dimensions.
	If you have historical data around user preferences (e.g. ratings of shoes), you can use a collaborative filter type approach. Mention specifically the rows and columns of the matrix you generate with either approach. Then discuss what kind of similarity metrics you would try. E.g. euclidean distance, Jaccard similarity, cosine distance.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Recommendation system and related}
\begin{block}{Question:}
	If you were a data scientist at a web company that sells shoes, how would you build a system that recommends shoes to a visitor? (Verizon interview)
\end{block}
\begin{block}{Answer:}
	After explaining the algorithmic aspect, you would discuss the data engineering side. Propose an engineering infrastructure that scales to millions of users/shoes where recommendations are generated in real time. As an example, you can stream the user data to a S3 bucket. You can perform the matrix analysis on a nightly basis, pre-calculate the entire set of recommendations on a per user basis, and store this in a in­memory database such as Redis. Then you could build a REST API that would query the database and respond with the recommendations given a user id.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{NLP questions and related}
\begin{block}{Question:}
	Given a set of historical news articles that have been classified as specific categories such as Sports, Politics, World, how would you classify a new article?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item Explore the data and understand key elements of the data.
		\begin{itemize}
			\item Plot the distribution of various categories in your training set to determine if there is label imbalance.
			\item Look at the text to identify anything strange, such as non-English text, heavy abbreviations, or misspellings.
			\item Do topic extraction to identify keywords for specific latent topics and find correlation to the labeled categories. This may give you a hint as to whether there are latent topics (keywords) that may correlate better than just using all the words.
		\end{itemize}
%		\item 
%		\item 
%		\item 
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{NLP questions and related}
\begin{block}{Question:}
	Given a set of historical news articles that have been classified as specific categories such as Sports, Politics, World, how would you classify a new article?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item Derive the training set by cleaning up the text. Remove lesser informative elements such as punctuation, abbreviations, and unicode characters. Do further cleaning by taking the lowercase of words and lemmatization/stemming.
		\item Use a TFIDF vectorizer to convert the data to a bag of words model with TFIDF metric. Set lower and upper bounds to TFIDF to reduce the vocabulary size.
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{NLP questions and related}
\begin{block}{Question:}
	Given a set of historical news articles that have been classified as specific categories such as Sports, Politics, World, how would you classify a new article?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item Build a pipeline where you can train various models and compare their performance relative to metrics such as AUC, F1 score, precision and recall. You can do gridsearch to automate the cross validation as well.
		\item Once you get the optimal model, you can publish this model to production using a pickled model (in python) or POJO (in java). This model can then be queried by using the exact same process of cleaning as done in previous steps for the new articles.
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Python basics: shallow copy vs. deep copy}
\begin{block}{Question:}
	What is difference between shallow copy and deep copy?
\end{block}
\begin{block}{Answer:}
	%	Assignment statements in Python do not copy objects, they create bindings between a target and an object.
	
	The difference between shallow and deep copying is only relevant for compound objects (objects that contain other objects, like lists or class instances)
	
	A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original.
	
	A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original.
	
	Shallow copies of dictionaries can be made using \texttt{dict.copy()}, and of lists by assigning a slice of the entire list, for example, \texttt{copied\_list = original\_list[:]}.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Python basics: tuples vs. lists}
\begin{block}{Question:}
	What is the difference between tuples and lists in Python?
\end{block}
\begin{block}{Answer:}
	Tuples can be used as keys for dictionaries i.e. they can be hashed. Lists are mutable whereas tuples are immutable  they cannot be changed. Tuples should be used when the order of elements in a sequence matters. For example, set of actions that need to be executed in sequence, geographic locations or list of points on a specific route.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Python basics: sort an Numpy array}
\begin{block}{Question:}
	Write the code to sort an array in NumPy by the nth column
\end{block}
\begin{block}{Answer:}
	Using \texttt{argsort()} function this can be achieved. If there is an array X and you would like to sort the nth column then code for this will be \\
	\texttt{x[x[:, n-1].argsort()]}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Python basics: lambda function}
\begin{block}{Question:}
	Can the lambda forms in Python contain statements?
\end{block}
\begin{block}{Answer:}
	No, as their syntax is restricted to single expressions and they are used for creating function objects which are returned at runtime.
	
	Python’s lambda forms cannot contain statements because Python’s syntactic framework can’t handle statements nested inside expressions.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Data structure and related}
\begin{block}{Question:}
	What are some differences between a linked list and an array?
\end{block}
\begin{block}{Answer:}
	An array is an ordered collection of objects. A linked list is a series of objects with pointers that direct how to process them sequentially. An array assumes that every element has the same size, unlike the linked list. A linked list can more easily grow organically: an array has to be pre-defined or re-defined for organic growth. Shuffling a linked list involves changing which points direct where — meanwhile, shuffling an array is more complex and takes more memory.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Data structure and related}
\begin{block}{Question:}
	Describe a hash table.
\end{block}
\begin{block}{Answer:}
	A hash table is a data structure that produces an associative array. A key is mapped to certain values through the use of a hash function. They are often used for tasks such as database indexing.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Behavior questions}
\begin{block}{Question:}
	What challenges does a Data Scientist usually face?
\end{block}
\begin{block}{Answer:}
	Handling data from multiple sources: The value of the data mined increases when a data scientist is able to reach across the expanse of the data landscape and access data from multiple platforms and data sources.
	
	Predicting Outcomes from the data: The final result is not necessarily the same as predicting data. Results might be unexpected even when datasets are given.
	
	Communicating with people: Understanding the data is not enough for a Data Scientist. It is also important to make it presentable to the people as well.
	
	Converting business problem into a analytical problem.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Behavior questions}
\begin{block}{Question:}
	While building a model, you typically use different Python packages like scikit-learn. If the organization says that you will have to develop the model yourself, how comfortable would you be in doing that?
\end{block}
\begin{block}{Answer:}
	This is a good way for interviewers to test for the team-collaboration spirit of the candidate and also if he can figure out a way given a particular situation. A good answer would be:
	This situation typically happens while training models offline and getting them into production, since the production system is not necessarily on Python i.e. say the deployment environment is Php. You need to work with developers to deploy them. Thankfully a lot of the open source code is available with the source code. 
	So, if one knows what the algorithm is doing, he can easily provide the pseudo code or the equations to the developers who can then implement them in Java or other languages.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Brain Teaser}
\begin{block}{Question:}
	Imagine you have $n$ pieces of rope in a bucket. You reach in and grab one end-piece, then reach in and grab another end-piece, and tie those two together. What is the expected value of the number of loops in the bucket?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item There are $n$ entirely unattached pieces of rope in a bucket
		\item A loop: any number of rope attached in a closed chain
		\item Let $L_{n-1}$ denote the expected number of loops for $n-1$ pieces
		\item Consider the bucket of $n$ pieces of rope, there are $2n$ rope ends
	\end{itemize}
Pick an end of rope, of the remaining $2n-1$ ends of rope only one end creates a loop(the other end of the same piece of rope). There are then $n-1$ untied pieces of rope. The rest of time, two separate pieces of rope are tied together and there are effectively $n-1$ untied pieces of rope. the recurrence is therefore:
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Brain Teaser}
\begin{block}{Question:}
	Imagine you have $n$ pieces of rope in a bucket. You reach in and grab one end-piece, then reach in and grab another end-piece, and tie those two together. What is the expected value of the number of loops in the bucket?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item $L_n = \frac{1}{2n-1} + L_{n-1}$, clearly $L_1 = 1$
		\item $L_n = \sum_{k=1}^{n}{\frac{1}{2k-1}}=H_{2n} - \frac{H_n}{2}$, where $H_k$ is $k$th harmonic number
		\item Since $H_k \approx \gamma + \ln{k}$ for large $k$, where $\gamma \approx 0.57722$, is the Euler-Mascheroni constant
		\item $L_n \approx \ln{2n} - \frac{\ln{n}}{2} = \ln{2\sqrt{n}}$
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Brain Teaser}
\begin{block}{Question:}
	In any 15-minute interval, there is a $20\%$ probability that you will see at least one shooting star. What is the proba­bility that you see at least one shooting star in the period of an hour?
\end{block}
\begin{block}{Answer:}
	Probability of not seeing any shooting star in 15 minutes is
	\begin{align*}
		1 - P(\text{Seeing at least one shooting star})= 1- 0.2=0.8
	\end{align*}
	Probability of not seeing any in the period of one hour is $0.8^4=0.4096$
	
	Probability of seeing at least one in one hour is
	\begin{align*}
	1 - P(\text{Not seeing any in one hour})= 1- 0.4096=0.5904
	\end{align*}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is root cause analysis?
\end{block}
\begin{block}{Answer:}
	Root cause analysis (RCA) is a method of problem-solving used for identifying the root causes of faults or problems. A factor is considered a root cause of removal thereof from the problem-fault-sequence prevents the final undesirable event from recurring; whereas a causal factor is one that affects an event's outcome, but is not a root cause. Though removing a causal factor can benefit an outcome, it does not prevent its recurrence with certainty.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	Why data cleaning plays a vital role in analysis?
\end{block}
\begin{block}{Answer:}
	Cleaning data from multiple sources to transform it into a format that data analysts or data scientists can work with is a cumbersome process because - as the number of data sources increases, the time take to clean the data increases exponentially due to the number of sources and the volume of data generated in these sources. It might take up to $80\%$ of the time for just cleaning data making it a critical part of analysis task.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is a point estimate?
\end{block}
\begin{block}{Answer:}
	In statistics, point estimation involves the use of sample data to calculate a single statistic which is to serve as a ‘best guess’ or ‘best estimate’ of an unknown (fixed or random) population parameter. There are a variety of point estimators, each with different properties.
	Minimum-variance mean-unbiased estimator (MVUE)
	Best linear unbiased estimator (BLUE)
	Minimum mean square error (MMSE)
	Median-unbiased estimator
	Maximum likelihood (ML)
	Method of moments, generalized method of moments
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is the difference between usual statistical analysis and time series analysis?  
\end{block}
\begin{block}{Answer:}
	Time series analysis accounts for the autocorrelation between time events, which is the next value in a time series is affected by the earlier values in the time series. Time series analysis accounts for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is probability distribution type you would use to describe the following random variables?
	a. Probability of k customers arriving to a restaurant within a duration of t minutes
	b. The probability of the height of a person in a crowd being at least X inches
\end{block}
\begin{block}{Answer:}
	A. Poisson distribution. This is assuming that customer arrivals are entirely independent from each other.
	
	B. Normal distribution. Note that in a continuous distribution the likelihood of being exactly X inches is zero.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is probability distribution type you would use to describe the following random variables?
	c. The probability of the sum of two 6­sided fair dices being Y
	d. The probability of having k heads thrown out of N coin throws
\end{block}
\begin{block}{Answer:}
	C. $P(sum(x1+x2) = \{0,1,(2,12),(3,11),(4,10)\ldots 36\}) = \{0,0,1/36,2/36,3/36,\ldots\}$
	
	D. Binomial distribution. 
	\[
		P(k \text{ heads in } N \text{ throws}) = \binom{N}{k} = \frac{N!}{k!\, (N-k)!}
	\]
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	How does wordcount map-reduce work on Hadoop?
\end{block}
\begin{block}{Answer:}
	The driver code would set up the job and configuration. If the data comes from HDFS and output is written to HDFS, add the input/output path to the job to those directories. Then the mapper job would take each line in the file and emit a value of 1 for each word as the key.
	
%	Note that the data passed between mapper and reducer must use the Hadoop data structures such as Text and IntWritables since these are more efficient for byte array serialization vs. primitive types such as Strings and Ints. The mapper output would then be collected in each executor, and then the combiner task would be executed. The combiner is a local aggregator that is optionally set to reduce the amount of data sent between the mappers and reducers.
	
	Once all the mappers are complete, only then can the shuffle phase begin. You might observe your jobs stuck at $33\%$ reducer, which implies that the shuffle phase is waiting on the mappers to complete. Once all the keys are sent to the reducers based on this shuffle, the sort phase begins on each reducer. After that, the reduce logic is executed, and the output can be written to another HDFS file.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What’s the difference between a generative and discriminative model?
\end{block}
\begin{block}{Answer:}
	A generative model will learn categories of data while a discriminative model will simply learn the distinction between different categories of data. Discriminative models will generally outperform generative models on classification tasks.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What cross-validation technique would you use on a time series dataset?
\end{block}
\begin{block}{Answer:}
	Time series is inherently ordered by chronological order. If a pattern emerges in later time periods for example, your model may still pick up on it even if that effect doesn’t hold in earlier years!
	
	You’ll want to do something like forward chaining where you’ll be able to model on past data then look at forward-facing data.
	
	fold 1 : training [1], test [2]
	
	fold 2 : training [1 2], test [3]
	
	fold 3 : training [1 2 3], test [4]
	
	fold 4 : training [1 2 3 4], test [5]
	
	fold 5 : training [1 2 3 4 5], test [6]
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	How do you ensure you’re not overfitting with a model?
\end{block}
\begin{block}{Answer:}
	There are three main methods to avoid overfitting:
	
	1- Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data.
	
	2- Use cross-validation techniques such as k-folds cross-validation.
	
	3- Use regularization techniques such as LASSO that penalize certain model parameters if they’re likely to cause overfitting.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What evaluation approaches would you work to gauge the effectiveness of a machine learning model?
\end{block}
\begin{block}{Answer:}
	You would first split the dataset into training and test sets, or perhaps use cross-validation techniques to further segment the dataset into composite sets of training and test sets within the data. You should then implement a choice selection of performance metrics: here is a fairly comprehensive list. You could use measures such as the F1 score, the accuracy, and the confusion matrix.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	How would you implement a recommendation system for our company’s users?
\end{block}
\begin{block}{Answer:}
	A lot of machine learning interview questions of this type will involve implementation of machine learning models to a company’s problems. You’ll have to research the company and its industry in-depth, especially the revenue drivers the company has, and the types of users the company takes on in the context of the industry it’s in.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	How can we use your machine learning skills to generate revenue?
\end{block}
\begin{block}{Answer:}
	This is a tricky question. The ideal answer would demonstrate knowledge of what drives the business and how your skills could relate. For example, if you were interviewing for music-streaming startup Spotify, you could remark that your skills at developing a better recommendation model would increase user retention, which would then increase revenue in the long run.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is the Central Limit Theorem? Explain it. Why is it important?
\end{block}
\begin{block}{Answer:}
	The CLT states that the arithmetic mean of a sufficiently large number of iterates of independent random variables will be approximately normally distributed regardless of the underlying distribution. i.e: the sampling distribution of the sample mean is normally distributed.
		\begin{itemize}
			\item Used in hypothesis testing
			\item Used for confidence intervals
			\item Random variables must be iid: independent and identically distributed
			\item Finite variance
		\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	Define: quality assurance, six sigma.
\end{block}
\begin{block}{Answer:}
	Quality assurance:
		\begin{itemize}
			\item A way of preventing mistakes or defects in manufacturing products or when delivering services to customers
			\item In a machine learning context: anomaly detection
		\end{itemize}
	Six sigma:
		\begin{itemize}
			\item Set of techniques and tools for process improvement
			\item $99.99966\%$ of products are defect-free products (3.4 per 1 million)
			\item 6 standard deviation from the process mean
		\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	How can we use your machine learning skills to generate revenue?
\end{block}
\begin{block}{Answer:}
	This kind of question requires you to listen carefully and impart feedback in a manner that is constructive and insightful. Your interviewer is trying to gauge if you’d be a valuable member of their team and whether you grasp the nuances of why certain things are set the way they are in the company’s data process based on company- or industry-specific conditions. They’re trying to see if you can be an intellectual peer. Act accordingly.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}



\end{document}

