\documentclass[11pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{enumitem}
\usepackage{graphicx}
\usetheme{EastLansing}
\author{Wang Yao}
\begin{document}
	\title{Data Science Study Notes}
	\subtitle{Some Common Interview Questions}
	%\logo{}
	%\institute{}
	%\date{}
	%\subject{}
	%\setbeamercovered{transparent}
	%\setbeamertemplate{navigation symbols}{}
	\begin{frame}[plain]
	\maketitle
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{block}{Question:}
	What are basic methods to reduce dimension?
\end{block}
\begin{itemize}
	\item Use correlation test to remove correlated numerical variables with business understanding
	\begin{itemize}
		\item A correlation coefficient measures the extent to which two variables tend to change together. The coefficient describes both  strength and direction of the relationship
		\item \textbf{Pearson correlation} evaluates the \textbf{linear relationship} between two \textbf{continuous variables}
		\item \textbf{Spearman rank-order correlation} evaluates the \textbf{monotonic relationship} between two continuous or \textbf{ordinal} variables. In a \textbf{monotonic relationship}, the variables tend to change together, but not necessarily at a constant rate. 
		\item The \textbf{Pearson} and \textbf{Spearman} coefficient are both nearly $0$ \textbf{does not} implies there is no strong relationship between two variables
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item  Use Chi-Square test to remove correlated categorical variables with business understanding
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item Use PCA and pick the components which can explain the maximum variance in the data set. 
	\begin{itemize}
		\item \textbf{Principal Component Analysis} performs a linear mapping of the data to a lower dimensional space in such a way that the variance of data in the low-dimensional representation is maximized. 
		\item In practice, the \textbf{convariance}(sometimes the \textbf{correlation}) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues can now be used to reconstruct a large fraction of the variance of original data.
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item \textbf{Kernel PCA}
	\begin{itemize}
		\item \textbf{PCA} can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in data.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item \textbf{Graph-based kernel PCA} is one of techniques that try to learn the kernel using semidefinite programming, instead of defining a fixed kernel.
	\begin{itemize}
		\item \textbf{PCA} can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in data.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correlation and PCA in dimension reduction}
\begin{itemize}
	\item \textbf{Non-negative matrix factorization(NMF)}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LDA, GDA and Autoencoder in dimension reduction}
\begin{itemize}
	\item \textbf{Linear discriminant analysis(LDA)}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LDA, GDA and Autoencoder in dimension reduction}
\begin{itemize}
	\item \textbf{Generalized discriminant analysis(GDA)}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LDA, GDA and Autoencoder in dimension reduction}
\begin{itemize}
	\item \textbf{Autoencoder}
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Kernel method}
\begin{itemize}
	\item In its simplest form, the \textbf{Kernel methods} means transforming data into another dimension that has a clear dividing margin between classes of data 
	\begin{itemize}
		\item \textbf{NMF} decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.
		\item In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Time Series and related}
\begin{block}{Question:}
	How can you check if a data set or time series is Random?
\end{block}
\begin{block}{Answer:}
	To check whether a dataset is random or not, use the lag plot. If the lag plot for the given dataset does not show any structure then it is random.
\end{block}
\begin{itemize}
%	\item A lag plot is a special type of scatter plot with the two variables $(X,Y)$ “lagged.” 
	\item One set of observations in a time series is plotted (lagged) against a set of later data. The kth lag is the time period that happened $k$ time points before time $i$. e.g.: $Lag_1(Y2) = Y1$ and $Lag_4(Y9) = Y5$.
	\begin{itemize}
		\item Model suitability and outliers
		\item Randomness (data without a pattern), Serial correlation (where error terms in a time series transfer from one period to another).
		\item Seasonality (periodic fluctuations in time series data that happens at regular periods).
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Clustering and related}
\begin{block}{Question:}
	How will you define the number of clusters in a clustering algorithm?
\end{block}
\begin{block}{Answer:}
	Though the Clustering Algorithm is not specified, this question is mostly in reference to $K$-Means clustering where $K$ defines the number of clusters. The objective of clustering is to group similar entities in a way that the entities within a group are similar to each other but the groups are different from each other
	
	Within Sum of squares is generally used to explain the homogeneity within a cluster. If you plot WSS for a range of number of clusters, you will get "Elbow Curve", Use "Bending Point" as $K$ in $K$-means, where within groups sum of squares does not decrease significantly.
\end{block}
%\begin{itemize}
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item 
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Time Series and related}
\begin{block}{Question:}
	How can you deal with different types of seasonality in time series modeling?
\end{block}
\begin{block}{Answer:}
	Seasonality in time series occurs when time series shows a repeated pattern over time. E.g., stationary sales decreases during holiday season, air conditioner sales increases during the summers etc.
	
	Seasonality makes your time series non-stationary because average value of the variables at different time periods. Differentiating a time series is generally known as the best method of removing seasonality from a time series. Seasonal differencing can be defined as a numerical difference between a particular value and a value with a periodic lag.
	
	Model the seasonal component directly and subtract it from observations.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item 
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Linear model and related}
\begin{block}{Question:}
	What’s the assumption of linear regression?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item Linear relationship between dependent and independent variables
		\item Statistical independence of the errors (no correlation between consecutive errors in the case of time series data)
		\item Homoscedasticity (constant variance) of the errors
		\item Normality of the error distribution.
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item 
%	\item 
%	\item
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Linear model and related}
\begin{block}{Question:}
	How to check if a distribution is close to Normal? Why would you want to check it? What is a QQ Plot?
\end{block}
\begin{block}{Answer:}
	A Q-Q plot or quantile-quantile plot, is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a Normal or exponential. 
	
	For example, if we run a statistical analysis that assumes our independent variable is Normally distributed, we can use a Normal Q-Q plot to check that assumption. It’s just a visual check, not an airtight proof, so it is somewhat subjective. 
	
	But it allows us to see at-a-glance if our assumption is plausible, and if not, how the assumption is violated and what data points contribute to the violation.
\end{block}
%\begin{itemize}
%	\item 
%	\item
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	Can you cite some examples where a false negative important than a false positive?
\end{block}
\begin{block}{Answer:}
	Assume there is an airport which has received high security threats and based on certain characteristics they try to identify whether a particular passenger can be a threat or not. Due to shortage of staff they decided to scan passenger being predicted as risk positives by their predictive model.
	What will happen if a true threat customer is being flagged as non-threat by airport model?
	
	Another example can be judicial system. What if Jury or judge decide to make a criminal go free?
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of sensitivity?
\end{block}
\begin{block}{Answer:}
	Sensitivity is also called true positive rate, recall, which measures the proportion of actual positives that are correctly identified as such(e.g., the percentage of sick people who are correctly identified as having the condition).
	
	Specificity is also called true negative rate, which measures the proportion of actual negatives that are correctly identified as such(e.g., the percentage of healthy people who are correctly identified as not having the condition).
	
	Sensitivity quantifies the avoiding of false negatives and specificity does the same for false positives. For any test, there is usually a trade-off between the measures
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of sensitivity?
\end{block}
\begin{block}{Answer:}
	Equivalently, in medical tests sensitivity is the extent to which actual positives are not overlooked.
	
	Specificity is the extent to which actual negatives are classified as such.
	
	Thus a highly sensitive test rarely overlooks an actual positive; a highly specific test rarely registers a positive classification for anything that is not the target of testing.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of sensitivity?
\end{block}
\begin{block}{Answer:}
	Sensitivity refers to the test's ability to correctly detect ill patients who do have the condition. In the example of a medical test used to identify a disease, the sensitivity of the test is the proportion of people who test positive for the disease among those who have the disease.
	\begin{align*}
		\text{sensitivity}=&\frac{\text{num. of true positives}}{\text{num. of true positives} +\text{num. of false negatives}}\\
		=&\frac{\text{num. of true positives}}{\text{total num. of sick individuals in population}}\\
		=& \text{prob. of a positive test given that the patient has the disease}
	\end{align*}

\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of  specificity?
\end{block}
\begin{block}{Answer:}
	Specificity relates to the test's ability to correctly reject healthy patients without a condition. In the example of a medical test for diagnosing a disease, specificity of a test is the proportion of healthy patients known not to have the disease, who will test negative for it.
	\begin{align*}
	\text{specificity}=&\frac{\text{num. of true negatives}}{\text{num. of true negatives} +\text{num. of false positives}}\\
	=&\frac{\text{num. of true negatives}}{\text{total num. of well individuals in population}}\\
	=& \text{prob. of a negative test given that the patient is well}
	\end{align*}
	
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of  specificity?
\end{block}
\begin{block}{Answer:}
	A positive result in a test with high specificity is useful for ruling in disease. The test rarely gives positive results in healthy patients. A test with $100\%$ specificity will read negative, and accurately exclude disease from all healthy patients. A positive result signifies a high probability of the presence of disease.
	
	A test with a higher specificity has a lower type I error rate.
	
	The tradeoff between specificity and sensitivity is explored in ROC analysis as a trade off between TPR and FPR
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of  specificity?
\end{block}
\begin{block}{Answer:}
	A positive result in a test with high specificity is useful for ruling in disease. The test rarely gives positive results in healthy patients. A test with $100\%$ specificity will read negative, and accurately exclude disease from all healthy patients. A positive result signifies a high probability of the presence of disease.
	
	A test with a higher specificity has a lower type I error rate.
	
	The tradeoff between specificity and sensitivity is explored in ROC analysis as a trade off between TPR and FPR
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of sensitivity?
\end{block}
\begin{block}{Answer:}
	The number of real positive(negative) events: $\textbf{P}$($\textbf{N}$)
	
	True positive(negative): $\textbf{TP}$($\textbf{TN}$); False positive(negative): $\textbf{FP}$($\textbf{FN}$)
	Sensitivity, Recall, Hit Rate or True Positive Rate(\textbf{TPR})
	\[
		\textbf{TPR} = \frac{\textbf{TP}}{\textbf{P}} = \frac{\textbf{TP}}{\textbf{TP}+\textbf{FN}}
		=1 - \textbf{FNR}
	\]
	Specificity, Selectivity or True Negative Rate(\textbf{TNR})
	\[
		\textbf{TNR} = \frac{\textbf{TN}}{\textbf{N}} = \frac{\textbf{TN}}{\textbf{FP}+\textbf{TN}}
		=1 - \textbf{FPR}
	\]
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of sensitivity?
\end{block}
\begin{block}{Answer:}
	Precision or Positive Predictive Value(\textbf{PPV})
	\[
		\textbf{PPV} = \frac{\textbf{TP}}{\textbf{TP}+\textbf{FP}}
	\]
	Negative Predictive Value(\textbf{NPV})
	\[
		\textbf{NPV} = \frac{\textbf{TN}}{\textbf{TN}+\textbf{FN}}
	\]
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	What do you understand by statistical power of sensitivity?
\end{block}
\begin{block}{Answer:}
	Accuracy(\textbf{ACC})
	\[
		\textbf{ACC} = \frac{\textbf{TP}+\textbf{TN}}{\textbf{P}+\textbf{N}} = \frac{\textbf{TP}+\textbf{TN}}{\textbf{TP}+\textbf{TN} + \textbf{FP}+\textbf{FN}}
	\]
	$\textbf{F}_1$ Score(harmonic mean of Precision and Recall)
	\[
		\textbf{F}_1 = 2\times\frac{\textbf{TPR}\times\textbf{PPV}}{\textbf{TPR} + \textbf{PPV}} = \frac{2\textbf{TP}}{2\textbf{TP}+\textbf{FP}+\textbf{FN}}
	\]
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confusion matrix and related}
\begin{block}{Question:}
	An HIV test has a sensitivity of $99.7\%$ and a specificity of $98.5\%$. A subject from a population of prevalence $0.1\%$ receives a positive test result. What is the precision of the test (i.e the probability he is HIV positive)?
\end{block}
\begin{block}{Answer:}
	\begin{align*}
	&P(Actu_+\mid Pred_+)\\ &\quad= \frac{P( Pred_+\mid Actu_+)\times P(Actu_+)}{P( Pred_+\mid Actu_+)\times P(Actu_+) + P( Pred_+\mid Actu_-)\times P(Actu_-)}\\
	&\quad=\frac{sensitivity\times prevalence}{sensitivity\times prevalence + (1 - specificity)\times(1-prevalence)}\\
%	&\quad=\frac{0.997\times 0.001}{0.997\times 0.001 + 0.015\times 0.999} = 0.062
	\end{align*}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model performance metrics and related}
\begin{block}{Question:}
	What are the limitations of $R^2$
\end{block}
\begin{block}{Answer:}
	$R^2$ is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in dependent variable that independent variables explain collectively. 
%	$R^2$ measures the strength of the relationship between your model and the dependent variable.
	$R^2$ cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots.
	$R^2$ does not indicate whether a regression model is adequate. You can have a low $R^2$ value for a good model, or a high $R^2$ value for a model that does not fit the data.
	$R^2$ does not measure goodness of fit.
	$R^2$ does not measure predictive error.
	$R^2$ does not allow you to compare models using transformed responses.
	$R^2$ does not measure how one variable explains another.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model validation and related}
\begin{block}{Question:}
	How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression.
\end{block}
\begin{block}{Answer:}
	If the values predicted by the model are far outside of the response variable range, this would immediately indicate poor estimation or model inaccuracy. If the values seem to be reasonable, examine the parameters; any of the following would indicate poor estimation or multi-collinearity: opposite signs of expectations, unusually large or small values, or observed inconsistency when the model is fed new data. Use the model for prediction by feeding it new data, and use the coefficient of determination ($R^2$) as a model validity measure mean squared error (MSE).
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Confidence interval}
\begin{block}{Question:}
	What is confidence interval?
\end{block}
\begin{block}{Answer:}
	In statistics, a confidence interval is a type of interval estimate that is computed from the observed data. A confidence interval gives an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a given set of sample data.
	
	The confidence level is the probability that the interval produced by the method employed includes the true value of the parameter. Confidence intervals consist of a range of values (interval) that act as good estimates of the unknown population parameter.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Long-tailed distribution}
\begin{block}{Question:}
	Explain what a long-tailed distribution is and provide examples. Why are they important in classification and regression problems?
\end{block}
\begin{block}{Answer:}
	In a skewed distribution with a long tail, a high frequency population is followed by a low frequency population, which gradually tails off asymptotically.
%	Rule of thumb: majority of occurrences (more than half, and when Pareto principles applies, $80\%$) are accounted for by the first $20\%$ items in the distribution. The least frequently occurring $80\%$ of items are more important as a proportion of the total population.
	
	Natural language:
	The frequency of any word is inversely proportional to its rank in the frequency table. 
%	The most frequent word will occur twice as often as the second most frequent, three times as often as the third most frequen.
	% “The” accounts for $7\%$ of all word occurrences (70000 over 1 million). “of” accounts for $3.5\%$, followed by “and”… Only 135 vocabulary items are needed to account for half the English corpus.
	
	Allocation of wealth among individuals: the larger portion of the wealth of any society is controlled by a smaller percentage of the people. 
	%File size distribution of Internet Traffic, Hard disk error rates, values of oil reserves in a field (a few large fields, many small ones), sizes of sand particles, sizes of meteorites.
	
	In classification and regression problems, this is a issue when using models that make assumptions on the linearity and need to apply a monotone transformation on the data (logarithm). When sampling, the data will become even more unbalanced.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Activation function in NN}
\begin{block}{Question:}
	What is the function of activation function in ANN?
\end{block}
\begin{block}{Answer:}
	To add the non-linear factor into the model.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sampling methods}
\begin{block}{Question:}
	What is sampling? How many sampling methods?
\end{block}
\begin{block}{Answer:}
	Sampling is that part of statistical practice concerned with the selection of an unbiased or random subset of individual observations within a population of individuals intended to yield some knowledge about the population of concern.
	
	There are four sampling methods: Simple Random (purely random), Systematic( every kth member of population), cluster (population divided into groups or clusters)and stratified (divided by exclusive groups or strata, sample from each group) samplings.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Loss function}
\begin{block}{Question:}
	What’s the loss function log-loss?
\end{block}
\begin{block}{Answer:}
	Log Loss is an evaluation metric used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier’s predictions.
	
	Maximizing the likelihood is the same as minimizing the cross entropy
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Missing value treatment}
\begin{block}{Question:}
	How do data management procedures like missing data handling make selection bias worse?
\end{block}
\begin{block}{Answer:}
	Removing entire row in data even if one value is missing could achieve a selection bias if your values are not missing at random and they have some pattern. 
	
	Replacing missing values with mean of other available values might make your distribution biased e.g., standard deviation, correlation and regression are mostly dependent on the mean value of variables.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Missing value treatment}
\begin{block}{Question:}
	During analysis, how do you treat missing values?
\end{block}
\begin{block}{Answer:}
	If any patterns for missing values are identified the analyst has to concentrate on them as it could lead to interesting and meaningful business insights. If there are no patterns identified, then the missing values can be substituted with mean or median (imputation) or they can simply be ignored.
	
	If it is a categorical variable, the default value(using business insight) can be assigned.
	
	If you have a distribution of data coming, for normal distribution give the mean value.
	
	If $80\%$ or more of the values for a variable are missing then you would be dropping the variable.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Python basics: shallow copy vs. deep copy}
\begin{block}{Question:}
	What is difference between shallow copy and deep copy?
\end{block}
\begin{block}{Answer:}
%	Assignment statements in Python do not copy objects, they create bindings between a target and an object.
	
	The difference between shallow and deep copying is only relevant for compound objects (objects that contain other objects, like lists or class instances)
	
	A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original.
	
	A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original.
	
	Shallow copies of dictionaries can be made using \texttt{dict.copy()}, and of lists by assigning a slice of the entire list, for example, \texttt{copied\_list = original\_list[:]}.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Experimental design and related}
\begin{block}{Question:}
	Design an experiment to figure out which web design alternative to use. Assume there have been no other experiments done and there is no knowledge of the user behavior.
\end{block}
\begin{block}{Answer:}
	Decide the number of samples/visits necessary to hit the necessary statistical significance (e.g. $95\%$). This can be done by using a $\chi^2$ test (if we are using a binomial random variable of clicking vs. not clicking) or a $z$-­test (if we are using a normally distributed random variable). 
	
	You can then use $p$­-value to identify whether the metric of the $B$ test is statistically significantly different from the metric of  baseline $A$ test. If it is and the metric is better, then the alternative site is the better choice.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Experimental design and related}
\begin{block}{Question:}
	Design an experiment to figure out which web design alternative to use. Assume there have been no other experiments done and there is no knowledge of the user behavior.
\end{block}
\begin{block}{Answer:}
	Some other issues:
	1. Identify potential biases due to interactions across pages. 
	2. Perform a A/A test which implies testing two random samples of visitors, and check if the distribution and metric of choice does not have a statistically significant difference. This will ensure the fairness of the $A/B$ test. An $A/A$ test ensures that your audience does not have a particular skew or bias and a randomized selection for an $A/B$ test will be statistically relevant
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Recommendation system and related}
\begin{block}{Question:}
	If you were a data scientist at a web company that sells shoes, how would you build a system that recommends shoes to a visitor? (Verizon interview)
\end{block}
\begin{block}{Answer:}
	If it is a new company that does not have much historical user data, go with item­ to item similarity. If the number of different items/shoes is extremely large, consider using matrix factorization techniques to reduce the dimensions.
	If you have historical data around user preferences (e.g. ratings of shoes), you can use a collaborative filter type approach. Mention specifically the rows and columns of the matrix you generate with either approach. Then discuss what kind of similarity metrics you would try. E.g. euclidean distance, Jaccard similarity, cosine distance.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Recommendation system and related}
\begin{block}{Question:}
	If you were a data scientist at a web company that sells shoes, how would you build a system that recommends shoes to a visitor? (Verizon interview)
\end{block}
\begin{block}{Answer:}
	After explaining the algorithmic aspect, you would discuss the data engineering side. Propose an engineering infrastructure that scales to millions of users/shoes where recommendations are generated in real time. As an example, you can stream the user data to a S3 bucket. You can perform the matrix analysis on a nightly basis, pre-calculate the entire set of recommendations on a per user basis, and store this in a in­memory database such as Redis. Then you could build a REST API that would query the database and respond with the recommendations given a user id.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{NLP questions and related}
\begin{block}{Question:}
	Given a set of historical news articles that have been classified as specific categories such as Sports, Politics, World, how would you classify a new article?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item Explore the data and understand key elements of the data.
		\begin{itemize}
			\item Plot the distribution of various categories in your training set to determine if there is label imbalance.
			\item Look at the text to identify anything strange, such as non-English text, heavy abbreviations, or misspellings.
			\item Do topic extraction to identify keywords for specific latent topics and find correlation to the labeled categories. This may give you a hint as to whether there are latent topics (keywords) that may correlate better than just using all the words.
		\end{itemize}
%		\item 
%		\item 
%		\item 
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{NLP questions and related}
\begin{block}{Question:}
	Given a set of historical news articles that have been classified as specific categories such as Sports, Politics, World, how would you classify a new article?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item Derive the training set by cleaning up the text. Remove lesser informative elements such as punctuation, abbreviations, and unicode characters. Do further cleaning by taking the lowercase of words and lemmatization/stemming.
		\item Use a TFIDF vectorizer to convert the data to a bag of words model with TFIDF metric. Set lower and upper bounds to TFIDF to reduce the vocabulary size.
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{NLP questions and related}
\begin{block}{Question:}
	Given a set of historical news articles that have been classified as specific categories such as Sports, Politics, World, how would you classify a new article?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item Build a pipeline where you can train various models and compare their performance relative to metrics such as AUC, F1 score, precision and recall. You can do gridsearch to automate the cross validation as well.
		\item Once you get the optimal model, you can publish this model to production using a pickled model (in python) or POJO (in java). This model can then be queried by using the exact same process of cleaning as done in previous steps for the new articles.
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Python basics: tuples vs. lists}
\begin{block}{Question:}
	What is the difference between tuples and lists in Python?
\end{block}
\begin{block}{Answer:}
	Tuples can be used as keys for dictionaries i.e. they can be hashed. Lists are mutable whereas tuples are immutable  they cannot be changed. Tuples should be used when the order of elements in a sequence matters. For example, set of actions that need to be executed in sequence, geographic locations or list of points on a specific route.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Python basics: sort an Numpy array}
\begin{block}{Question:}
	Write the code to sort an array in NumPy by the nth column
\end{block}
\begin{block}{Answer:}
	Using \texttt{argsort()} function this can be achieved. If there is an array X and you would like to sort the nth column then code for this will be \\
	\texttt{x[x[:, n-1].argsort()]}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Python basics: lambda function}
\begin{block}{Question:}
	Can the lambda forms in Python contain statements?
\end{block}
\begin{block}{Answer:}
	No, as their syntax is restricted to single expressions and they are used for creating function objects which are returned at runtime.
	
	Python’s lambda forms cannot contain statements because Python’s syntactic framework can’t handle statements nested inside expressions.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Behavior questions}
\begin{block}{Question:}
	What challenges does a Data Scientist usually face?
\end{block}
\begin{block}{Answer:}
	Handling data from multiple sources: The value of the data mined increases when a data scientist is able to reach across the expanse of the data landscape and access data from multiple platforms and data sources.
	
	Predicting Outcomes from the data: The final result is not necessarily the same as predicting data. Results might be unexpected even when datasets are given.
	
	Communicating with people: Understanding the data is not enough for a Data Scientist. It is also important to make it presentable to the people as well.
	
	Converting business problem into a analytical problem.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Behavior questions}
\begin{block}{Question:}
	While building a model, you typically use different Python packages like scikit-learn. If the organization says that you will have to develop the model yourself, how comfortable would you be in doing that?
\end{block}
\begin{block}{Answer:}
	This is a good way for interviewers to test for the team-collaboration spirit of the candidate and also if he can figure out a way given a particular situation. A good answer would be:
	This situation typically happens while training models offline and getting them into production, since the production system is not necessarily on Python i.e. say the deployment environment is Php. You need to work with developers to deploy them. Thankfully a lot of the open source code is available with the source code. 
	So, if one knows what the algorithm is doing, he can easily provide the pseudo code or the equations to the developers who can then implement them in Java or other languages.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Brain Teaser}
\begin{block}{Question:}
	Imagine you have $n$ pieces of rope in a bucket. You reach in and grab one end-piece, then reach in and grab another end-piece, and tie those two together. What is the expected value of the number of loops in the bucket?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item There are $n$ entirely unattached pieces of rope in a bucket
		\item A loop: any number of rope attached in a closed chain
		\item Let $L_{n-1}$ denote the expected number of loops for $n-1$ pieces
		\item Consider the bucket of $n$ pieces of rope, there are $2n$ rope ends
	\end{itemize}
Pick an end of rope, of the remaining $2n-1$ ends of rope only one end creates a loop(the other end of the same piece of rope). There are then $n-1$ untied pieces of rope. The rest of time, two separate pieces of rope are tied together and there are effectively $n-1$ untied pieces of rope. the recurrence is therefore:
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Brain Teaser}
\begin{block}{Question:}
	Imagine you have $n$ pieces of rope in a bucket. You reach in and grab one end-piece, then reach in and grab another end-piece, and tie those two together. What is the expected value of the number of loops in the bucket?
\end{block}
\begin{block}{Answer:}
	\begin{itemize}
		\item $L_n = \frac{1}{2n-1} + L_{n-1}$, clearly $L_1 = 1$
		\item $L_n = \sum_{k=1}^{n}{\frac{1}{2k-1}}=H_{2n} - \frac{H_n}{2}$, where $H_k$ is $k$th harmonic number
		\item Since $H_k \approx \gamma + \ln{k}$ for large $k$, where $\gamma \approx 0.57722$, is the Euler-Mascheroni constant
		\item $L_n \approx \ln{2n} - \frac{\ln{n}}{2} = \ln{2\sqrt{n}}$
	\end{itemize}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Brain Teaser}
\begin{block}{Question:}
	In any 15-minute interval, there is a $20\%$ probability that you will see at least one shooting star. What is the proba­bility that you see at least one shooting star in the period of an hour?
\end{block}
\begin{block}{Answer:}
	Probability of not seeing any shooting star in 15 minutes is
	\begin{align*}
		1 - P(\text{Seeing at least one shooting star})= 1- 0.2=0.8
	\end{align*}
	Probability of not seeing any in the period of one hour is $0.8^4=0.4096$
	
	Probability of seeing at least one in one hour is
	\begin{align*}
	1 - P(\text{Not seeing any in one hour})= 1- 0.4096=0.5904
	\end{align*}
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is root cause analysis?
\end{block}
\begin{block}{Answer:}
	Root cause analysis (RCA) is a method of problem-solving used for identifying the root causes of faults or problems. A factor is considered a root cause of removal thereof from the problem-fault-sequence prevents the final undesirable event from recurring; whereas a causal factor is one that affects an event's outcome, but is not a root cause. Though removing a causal factor can benefit an outcome, it does not prevent its recurrence with certainty.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	Why data cleaning plays a vital role in analysis?
\end{block}
\begin{block}{Answer:}
	Cleaning data from multiple sources to transform it into a format that data analysts or data scientists can work with is a cumbersome process because - as the number of data sources increases, the time take to clean the data increases exponentially due to the number of sources and the volume of data generated in these sources. It might take up to $80\%$ of the time for just cleaning data making it a critical part of analysis task.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is a point estimate?
\end{block}
\begin{block}{Answer:}
	In statistics, point estimation involves the use of sample data to calculate a single statistic which is to serve as a ‘best guess’ or ‘best estimate’ of an unknown (fixed or random) population parameter. There are a variety of point estimators, each with different properties.
	Minimum-variance mean-unbiased estimator (MVUE)
	Best linear unbiased estimator (BLUE)
	Minimum mean square error (MMSE)
	Median-unbiased estimator
	Maximum likelihood (ML)
	Method of moments, generalized method of moments
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is the difference between usual statistical analysis and time series analysis?  
\end{block}
\begin{block}{Answer:}
	Time series analysis accounts for the autocorrelation between time events, which is the next value in a time series is affected by the earlier values in the time series. Time series analysis accounts for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is probability distribution type you would use to describe the following random variables?
	a. Probability of k customers arriving to a restaurant within a duration of t minutes
	b. The probability of the height of a person in a crowd being at least X inches
\end{block}
\begin{block}{Answer:}
	A. Poisson distribution. This is assuming that customer arrivals are entirely independent from each other.
	
	B. Normal distribution. Note that in a continuous distribution the likelihood of being exactly X inches is zero.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	What is probability distribution type you would use to describe the following random variables?
	c. The probability of the sum of two 6­sided fair dices being Y
	d. The probability of having k heads thrown out of N coin throws
\end{block}
\begin{block}{Answer:}
	C. $P(sum(x1+x2) = \{0,1,(2,12),(3,11),(4,10)\ldots 36\}) = \{0,0,1/36,2/36,3/36,\ldots\}$
	
	D. Binomial distribution. 
	\[
		P(k \text{ heads in } N \text{ throws}) = \binom{N}{k} = \frac{N!}{k!\, (N-k)!}
	\]
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Miscellaneous}
\begin{block}{Question:}
	How does wordcount map-reduce work on Hadoop?
\end{block}
\begin{block}{Answer:}
	The driver code would set up the job and configuration. If the data comes from HDFS and output is written to HDFS, add the input/output path to the job to those directories. Then the mapper job would take each line in the file and emit a value of 1 for each word as the key.
	
%	Note that the data passed between mapper and reducer must use the Hadoop data structures such as Text and IntWritables since these are more efficient for byte array serialization vs. primitive types such as Strings and Ints. The mapper output would then be collected in each executor, and then the combiner task would be executed. The combiner is a local aggregator that is optionally set to reduce the amount of data sent between the mappers and reducers.
	
	Once all the mappers are complete, only then can the shuffle phase begin. You might observe your jobs stuck at $33\%$ reducer, which implies that the shuffle phase is waiting on the mappers to complete. Once all the keys are sent to the reducers based on this shuffle, the sort phase begins on each reducer. After that, the reduce logic is executed, and the output can be written to another HDFS file.
\end{block}
%\begin{itemize}
%	\item
%	\item 
%	\item 
%	\item 
%	\begin{itemize}
%		\item 
%		\item 
%		\item
%	\end{itemize}
%\end{itemize}
\end{frame}




\end{document}

